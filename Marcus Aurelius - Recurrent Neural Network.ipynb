{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generator Notebook\n",
    "### _This is a developing story, check back for updates._\n",
    "\n",
    "The goal is to write as a stoic person. What Marcus Aurelius wrote is our input to learn how he wrote his meditations and to predict his next chapter.\n",
    "\n",
    "## Setup\n",
    "### Import Tensorflow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code created by: Carlos Utrilla Guerrero\n",
    "# Code source: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('meditations.mb.txt', 'http://classics.mit.edu/Antoninus/meditations.mb.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read first data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 244067 characters\n"
     ]
    }
   ],
   "source": [
    "# read, then decode for py2 compatibility\n",
    "text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n",
    "# length of text is the number of characters on it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided by The Internet Classics Archive.\n",
      "See bottom for copyright. Available online at\n",
      "    http://classics.mit.edu//Antoninus/meditations.html\n",
      "\n",
      "The Meditations\n",
      "By Marcus Aurelius\n",
      "\n",
      "\n",
      "Translated by Geo\n"
     ]
    }
   ],
   "source": [
    "# see first 200 characters\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters 74\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '4', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Check the unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('unique characters {}'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the text\n",
    "As map strings to numerical list. Create two vlookup tables as one mapping the characters to numbers, and the other from numbers to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '1': 12, '2': 13, '4': 14, '9': 15, ':': 16, ';': 17, '?': 18, '@': 19, 'A': 20, 'B': 21, 'C': 22, 'D': 23, 'E': 24, 'F': 25, 'G': 26, 'H': 27, 'I': 28, 'J': 29, 'K': 30, 'L': 31, 'M': 32, 'N': 33, 'O': 34, 'P': 35, 'Q': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'V': 41, 'W': 42, 'X': 43, 'Y': 44, 'Z': 45, '[': 46, ']': 47, 'a': 48, 'b': 49, 'c': 50, 'd': 51, 'e': 52, 'f': 53, 'g': 54, 'h': 55, 'i': 56, 'j': 57, 'k': 58, 'l': 59, 'm': 60, 'n': 61, 'o': 62, 'p': 63, 'q': 64, 'r': 65, 's': 66, 't': 67, 'u': 68, 'v': 69, 'w': 70, 'x': 71, 'y': 72, 'z': 73}\n",
      "['\\n' ' ' '!' '\"' \"'\" '(' ')' ',' '-' '.' '/' '0' '1' '2' '4' '9' ':' ';'\n",
      " '?' '@' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P'\n",
      " 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' '[' ']' 'a' 'b' 'c' 'd' 'e' 'f'\n",
      " 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x'\n",
      " 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "char2idx= {u:i for i, u in enumerate(vocab)}\n",
    "idx2char=np.array(vocab)\n",
    "print(char2idx)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 65, 62, ..., 26,  9,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int=np.array([char2idx[c] for c in text])\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We mapped char to int and we mapped the character as indexes from 0 to len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " '\\n':   0,\n",
      " ' ' :   1,\n",
      " '!' :   2,\n",
      " '\"' :   3,\n",
      " \"'\" :   4,\n",
      " '(' :   5,\n",
      " ')' :   6,\n",
      " ',' :   7,\n",
      " '-' :   8,\n",
      " '.' :   9,\n",
      " '/' :  10,\n",
      " '0' :  11,\n",
      " '1' :  12,\n",
      " '2' :  13,\n",
      " '4' :  14,\n",
      " '9' :  15,\n",
      " ':' :  16,\n",
      " ';' :  17,\n",
      " '?' :  18,\n",
      " '@' :  19,\n",
      " 'A' :  20,\n",
      " 'B' :  21,\n",
      " 'C' :  22,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(23)):\n",
    "    print(' {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Provided by T' ---- char mapped to int ---> [35 65 62 69 56 51 52 51  1 49 72  1 39]\n"
     ]
    }
   ],
   "source": [
    "#print how the first 13 chars from text are mapped to integ\n",
    "print('{} ---- char mapped to int ---> {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction task\n",
    "The forecasting task we try to perform is: Given a char, better a sequence of char, what is the most probable next char? \n",
    "\n",
    "+ Inputs: sequence of char\n",
    "\n",
    "+ Train: model to predict the output\n",
    "\n",
    "+ Output: following char at each time step\n",
    "\n",
    "| Model Specif | - Recurrent depends on the previously seen elements, given all char computed until this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training examples and targets\n",
    "Next divide the text into examples of sequences. Each input sequence will contain seq_length characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one char to the right.\n",
    "\n",
    "So break the text into chunks of seq_length+1. For instance, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\" and target is \"ello\".\n",
    "\n",
    "To do so, use ```tf.data.Dataset.from_tensor_slices``` function to convert vectorize text into a stream of character indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "# The max length sentence we want for a single input of char\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```batch``` method allowed us to convert these individual chars to sequences of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Provided by The Internet Classics Archive.\\nSee bottom for copyright. Available online at\\n    http://c'\n",
      "'lassics.mit.edu//Antoninus/meditations.html\\n\\nThe Meditations\\nBy Marcus Aurelius\\n\\n\\nTranslated by Georg'\n",
      "'e Long\\n\\n----------------------------------------------------------------------\\n\\nBOOK ONE\\n\\nFrom my gra'\n",
      "'ndfather Verus I learned good morals and the government\\nof my temper. \\n\\nFrom the reputation and remem'\n",
      "'brance of my father, modesty and a manly\\ncharacter. \\n\\nFrom my mother, piety and beneficence, and abst'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1,drop_remainder = True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text using ```map``` method to apply a simple function to each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset= sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print examples input and target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: 'Provided by The Internet Classics Archive.\\nSee bottom for copyright. Available online at\\n    http://'\n",
      "Target data: 'rovided by The Internet Classics Archive.\\nSee bottom for copyright. Available online at\\n    http://c'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data:', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each index of these vectors are processed as one time step. For input step 0, model recieves index for 'F' and tries to predict index 'i' as the next character. Next step, it does the same, the **RNN** considers the previous step in addition to the current input character thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 35  ('P')\n",
      "  expected output: 65  ('r')\n",
      "Step    1\n",
      "  input: 65  ('r')\n",
      "  expected output: 62  ('o')\n",
      "Step    2\n",
      "  input: 62  ('o')\n",
      "  expected output: 69  ('v')\n",
      "Step    3\n",
      "  input: 69  ('v')\n",
      "  expected output: 56  ('i')\n",
      "Step    4\n",
      "  input: 56  ('i')\n",
      "  expected output: 51  ('d')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print('Step {:4d}'.format(i))\n",
    "    print('  input: {}  ({:s})'.format(input_idx, repr(idx2char[input_idx])))\n",
    "    print('  expected output: {}  ({:s})'.format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training batches\n",
    "It uses ```tf.data``` to split the text into manageable sequences. We need to shuffle data and pack it into batches and eventually feeding this data into model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "# buffer size to shuffle the dataset. Amount of time allocate to process the data\n",
    "BUFFER_SIZE = 10000 # temp computer memory assigned\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build The Model\n",
    "Use ```tf.keras.Sequential``` to define the model. In that case, three layers are used to define the model:\n",
    "* ```tf.keras.layers.Embedding``` : The input layer. A training vlookup table that will map numbers of each chars to a vector with ```embedding_dim``` dimensions;\n",
    "* ```tf.keras.layers.GRU```: A type of RNN with size ```units=rnn_units```\n",
    "* ```tf.keras.layers.Dense```: The output layer with ```vocab_size``` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# Embedding dimension\n",
    "embedding_dim = 256\n",
    "# number of RNN units\n",
    "rnn_units=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size,embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size,embedding_dim,\n",
    "                                 batch_input_shape=[batch_size,None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                           return_sequences=True,\n",
    "                           stateful=True,\n",
    "                           recurrent_initializer = 'glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "vocab_size=len(vocab),\n",
    "embedding_dim=embedding_dim,\n",
    "rnn_units=rnn_units,\n",
    "batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model\n",
    "It is time to run the model and check if it behaves as expected. First check the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 74) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           18944     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 74)            75850     \n",
      "=================================================================\n",
      "Total params: 4,033,098\n",
      "Trainable params: 4,033,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual predictions from the model, we need to sample from the output distribution, to get actual indices. The distribution is defined by the logits over the character vocabulary. Try for the first example of the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That will give us, at each timestep, a prediction of the next character index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71, 66,  6,  1, 71, 17,  8, 45, 30, 13,  4,  1, 51, 11, 43, 68,  8,\n",
       "       39, 49, 58, 30, 39, 63, 34, 30, 61, 12,  5, 14, 13, 14, 34, 12, 14,\n",
       "       68, 56, 26, 48,  0, 18, 71, 26, 25, 60, 66, 13, 14, 32,  0, 67, 67,\n",
       "       45, 60, 49, 21, 18, 15, 39, 57, 63, 66, 43,  3, 19, 43, 56, 61, 20,\n",
       "       42,  3, 39, 12,  1,  1, 29, 38, 33,  7, 44, 67, 68, 65, 52, 70, 56,\n",
       "       56,  4, 53, 23, 43, 13, 22, 35, 60, 67, 51, 31, 36, 24, 15],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should decode this prediction by this untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " ' art calling out on the Rostra, hast thou forgotten, man,\\nwhat these things are?- Yes; but they are '\n",
      "\n",
      "Next Char Predictions: \n",
      " 'xs) x;-ZK2\\' d0Xu-TbkKTpOKn1(424O14uiGa\\n?xGFms24M\\nttZmbB?9TjpsX\"@XinAW\"T1  JSN,Yturewii\\'fDX2CPmtdLQE9'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\",repr(''.join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "At this point the model can be treated as typical classification model. Given RNN state, and the input this time step, predict the next class of the character.\n",
    "\n",
    "#### Attach an optimizer, and a loss function\n",
    "```tf.keras.losses.sparse_categorical_crossentropy``` loss function works. Also ```from logits``` need to be set cause return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (64, 100, 74) # (batch_size, sequence_length, vocab_size)\n",
      "Scalar_loss:       4.3045197\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print('Prediction shape:', example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')\n",
    "print('Scalar_loss:      ', example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Checkpoints\n",
    "Use ```tf.keras.callbacks.ModelCheckpoint``` to ensure check are done and sabed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the training as EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep training time reasonable, use 10 epochs to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 37 steps\n",
      "Epoch 1/10\n",
      "37/37 [==============================] - 92s 2s/step - loss: 3.4560\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 93s 3s/step - loss: 2.4541\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 95s 3s/step - loss: 2.2143\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 95s 3s/step - loss: 2.0833\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 95s 3s/step - loss: 1.9640\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 96s 3s/step - loss: 1.8495\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 97s 3s/step - loss: 1.7421\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 96s 3s/step - loss: 1.6404\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 96s 3s/step - loss: 1.5556\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 96s 3s/step - loss: 1.4791\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text\n",
    "For sake of simplicity, use a batch size of 1. To run model on different batch_size, we need to rebuild and restore the weights from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            18944     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 74)             75850     \n",
      "=================================================================\n",
      "Total params: 4,033,098\n",
      "Trainable params: 4,033,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The prediction loop\n",
    "_TO BE DEFINED_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step: generating text using learned model\n",
    "    \n",
    "    # number of characters to generate\n",
    "    num_generate = 2000\n",
    "    \n",
    "    # Converting our start string to numbers (vectoring)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "    \n",
    "    \n",
    "    # Here batch size equal to 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        \n",
    "        # using categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions/temperature\n",
    "        predicted_id= tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # pass predicted value as next input to the model\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARCUS AURELIUS: dodsing of whatever than it is in thow prward purpicion consider thyself suther which thou not suppes therseffer goods would and chelle toghing food\n",
      "no lave one suisen everything, shilled or sticl diplive and such dory which destrouber and happens weore?\n",
      "And of the dosteshis dispraticus and musiner,\n",
      "whan it wised mand, thun. For hy wast fen in\n",
      "Graming with past now porect, thou stan's not however has geen amout percuplecepore, time out for they deaphing, than\n",
      "this things,\n",
      "aptears\n",
      "that is thou is make breat\n",
      "mapely and distur's ippostent of quire to time any own atalt, but appect doe\n",
      "\n",
      "hould main from tilf andyily,\n",
      "to list does not end,\n",
      "toung way less pains pheserver that mat is a soling the hasm govity. \n",
      "\n",
      "Lot juth the wholate (his part awnow perpor abstamenter,\n",
      "butity. \n",
      "\n",
      "Fro most: and consting and purpoce,\n",
      "but elly whice netrinuver, then forme of a buture ussalls kind, of the expucted bain, which ase meen such many ress what hap others, is a\n",
      "commen misture\n",
      "the waurth something eather shule turnimy, and all thet which profer toom, and\n",
      "in a Hast be of these from somemf to thyself to things, and pprose: What they uccoding to thr poor donedom to carmour everon, and sabdire mong in the ward aprivine bo a way\n",
      "of chance powernd bodn aut dreak eppice ot doe, and so nature. nor social, and ablitstence oth rese verys and way an a man. Let thingaring with marm benour inso it nase reists it\n",
      "man appleat\n",
      "to raterm lifes himage and\n",
      "afforts, which is accorvine to reaponce of monewt hal. Letertaing, thou arsourm to naides of may harms, and shall bo inderide. And in the stien that show to\n",
      "came for the men with theo plaise thy power. And lifel that him reason and dis beximes. What I surd dible:. And what I sud a man, any ano de what is free conrtainst in his mun is a quity of was a waun purise, nor a man sus pusfus principle without fan and ruturn of tame thy arplowance intowents it pursuce; and that is deards, and\n",
      "things all things har I naturuly ancery tonthing. For it is where is th\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string =u\"MARCUS AURELIUS:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional features:\n",
    "https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
